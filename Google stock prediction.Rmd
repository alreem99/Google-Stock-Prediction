---
title: "R Notebook"
output: html_notebook
---

Sample of row
```{r}
dataset = read.csv('GOOG.csv') 
```

```{r}
View(dataset)
```

we removed the attributes (symbol, divCash, splitFactor) as they have one value only so we do not need them
```{r}
dataset=dataset[,2:12]
```

```{r}
print(dataset)
str(dataset)
```

Convert the date column to a date format
```{r}
dataset$date <- as.Date(dataset$date, format = "%Y-%m-%d %H:%M:%S")
```


#statiscal summarise
```{r}
summary(dataset)
```

#mean of closing price
Using the mean closing price can serve as a basic reference point or a simple benchmark for forecasting future stock prices. The mean closing price is the average price at which a stock has closed over a specific period.

```{r}
mean(dataset$close)
```

# variance Code
The concept of variance in the context of closing prices for stock prediction serves to quantify the spread or dispersion of the closing prices around their mean or average value. It provides a measure of how much the actual closing prices deviate from the average closing price over a specific period.
```{r}
var(dataset$close)
```

#Graphs codes

#Scatter Plot
```{r}
with(dataset, plot(volume, close))
```

#Barplot
```{r}
barplot(height = dataset$close, names.arg = dataset$date, xlab = "Date", ylab = "Closing price", main = "date vs Close")
```

#Histogram for Close
```{r}
hist(dataset$close)
```

#Data preprocessing 

#Data cleaning
# to find the total null values in the dataset
#Checking NULL, FALSE means no null, TRUE cells means the value of the cell is null

```{r}
is.na(dataset)
sum(is.na(dataset))

print("Since there is no NULL values we don't need to remove any rows")
```

#Detecting outliers

#removing close outlier
```{r}
library(outliers)

OutClose = outlier(dataset$close, logical =TRUE)
sum(OutClose)
Find_outlier = which(OutClose ==TRUE, arr.ind = TRUE)
OutClose
Find_outlier
```

#removing volume's outlier
```{r}
OutVolume = outlier(dataset$volume, logical =TRUE)
sum(OutVolume)
Find_outlier = which(OutVolume ==TRUE, arr.ind = TRUE)
OutVolume
Find_outlier
```


#Remove outlier
```{r}
dataset= dataset[-Find_outlier,]
```

#data before preprocessing
Before preprocessing the data for stock price prediction, several initial steps are undertaken to understand, review, and prepare the dataset. These steps are crucial for getting an overview of the data and assessing its quality and relevance for the prediction task.
```{r}
View(dataset)
print(dataset)
```


#transformation

#Feature selection ,Remove Redundant Features
```{r}
# load the library        
library(mlbench)
library(caret)
library(ggplot2)
library(lattice)

# calculate correlation matrix
correlationMatrix <- cor(dataset[,2:11])

# summarize the correlation matrix
print(correlationMatrix)

# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.5 )

# print indexes of highly correlated attributes
print(highlyCorrelated)
```

#Normalization
normalization was performed to ensure consistent scaling of the data. 
The normalization technique applied was the max-min normalization. This technique rescales the values of specific attributes within a defined range between 0 and 1. 

We can use the normalized dataset provides a more uniform and comparable representation of the attributes, enabling accurate analysis and modeling for stock predaction with result as shown.

```{r}
normalize <- function(x) {return ((x - min(x)) / (max(x) - min(x)))}
dataWithoutNormalization <- dataset
dataset$close<-normalize(dataWithoutNormalization$close)
dataset$volume<-normalize(dataWithoutNormalization$volume)
dataset$open<-normalize(dataWithoutNormalization$open)
dataset$low <-normalize(dataWithoutNormalization$low)
dataset$high <-normalize(dataWithoutNormalization$high)
```

#Discretization
we used the Discretization technique on our class label "close" to simplify it as it has a large continuous values, we made them fall into intervals, to make it easier to analyze
```{r}
dataset$close <- ifelse(dataset$close <= 0.2957251 , "low","High")

print(dataset)
```
we discretized it into two categories (low, high) based on the maen, low meaning it is less than the mean of the close , and high meaning it is equal to or higher than the mean.

#Encoding
We encoded close data into factors, which would help the model read this data easily
```{r}

dataset$close <- factor(dataset$close,levels = c("low", "High"), labels = c("1", "2"))

print(dataset)
```
#summary after preprocessing
after preprocessing the data for stock price prediction, several steps are taken to refine, clean, and prepare the data for analysis and modeling. These preprocessing steps aim to enhance the quality and reliability of the data for more accurate stock price prediction.

```{r}
print(dataset)
View(dataset)
```
Feature selection

Feature selection is a process of selecting a subset of relevant features (or attributes) from the original set of features in a dataset. The goal of feature selection is to choose the most relevant and important features, thereby reducing dimensionality, and improving model performance.

#Feature selection ,Feature selection using Recursive Feature Elimination or RFE
```{r}
    library(mlbench)
library(caret)

# define the control using a random forest selection function 
# number=12 means the length of the list
control <- rfeControl(functions=rfFuncs, method="cv", number=11)
# run the RFE algorithm from column 1 to 11  
results <- rfe(dataset[,1:10],dataset[,11], sizes=c(1:10), rfeControl=control)
```
# summarize the results
```{r}
print(results)
```

# list the chosen features
```{r}
predictors(results)
```

# plot the results
```{r}
plot(results, type=c("h", "o"))
```
Data Mining Task

We did both supervised and unsupervised learning techniques on our dataset (Google stock prediction), which involves classification and clustering methods, for classification we did a partitioning method called the train-test split, which splits the dataset into two subsets of different ratios, and we implemented three algorithms to form 9 different decision trees.

#Classification:

We will choose the attributes with the highest importance (from feature selection) to create a tree:

1. Dividing the dataset:

we divided our dataset into two divisions for each split:

first one  70-30, which means Training(70%) and Testing(30%):
```{r}
# a fixed random seed to make results reproducible
set.seed(1234)

# 1.Split the datasets into two subsets: Training(70%) and Testing(30%):
ind1 <- sample(c(TRUE, FALSE), nrow(dataset), replace=TRUE, prob=c(0.7,0.3))
trainData1  <- dataset[ind1==1,]
testData1 <- dataset[ind1==2,]
```

# 2.Determine the predictor attributes and the class label attribute.( the formula):
```{r}
library(party)    
#myFormula 
myFormula <- close ~volume+open+high+low

```

# 3.Build a decision tree using training set and check the Prediction:
```{r}
dataset_ctree <- ctree(myFormula, data=trainData1)
table(predict(dataset_ctree), trainData1$close)
# 4.Print and plot the tree:

print(dataset_ctree)
plot(dataset_ctree, type="simple")
```


```{r}
# 5.Use the constructed model to predict the class labels of test data:
testPred <- predict(dataset_ctree, newdata = testData)
result<-table(testPred, testData$close)
result
```
#CART tree: 70% training and 30% testing
```{r}
set.seed(15687)
sample<-sample.int(n=nrow(dataset),size=floor(0.7*nrow(dataset)), replace=F)
trainCart70<-dataset[sample,]
testCart30<-dataset[-sample,]
trainCart70<-trainCart70[,c(7:10)]
testCart30<-testCart30[,c(7:10)]
```

```{r}
library(rpart)
library(rpart.plot)


close.test=testCart30$close
fit.tree = rpart(close~ ., data=trainCart70, method = "class", cp=0)
fit.tree
rpart.plot(fit.tree)

fit.tree$variable.importance
pred.tree = predict(fit.tree, testCart30, type = "class")
table(pred.tree,close.test)
printcp(fit.tree)

```


```{r}
#Accuracy/precision/sensitivity of model is given by
accuracy70=((1569+  466 + 1341)/nrow(testCart30))*100
precision70=(((1569/2535)+(466/636)+(1341/1536))/3)*100
sensitivity70=(((1569/1575)+(321/1529)+(875/1603))/3)*100
specificity70=((((466+166+193+1341)/3132)+((1569+2+96+1341)/3178)+((1033+600+3+323)/3104))/3)*100
```

