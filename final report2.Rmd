---
title: "R Notebook"
output: html_notebook
editor_options: 
  markdown: 
    wrap: 72
---

# **Closing price of Google Stock Prediction**

1.  Problem

Predicting the closing price of a stock is a complex problem because of
several challenges: Complexity of Factors: Stock prices are influenced
by a multitude of factors such as market trends, Analyzing and
incorporating all these factors accurately into a predictive model is a
complex task. Market Volatility This volatility makes predicting stock
prices accurately challenging. Data Quality and Quantity The pursuit of
solving this problem is crucial because accurate stock price predictions
have significant implications for investors, financial institutions, and
businesses. Accurate predictions can aid investors in making informed
decisions. The importance of predicting stock prices lies in its
implications for investors, financial institutions, and businesses.
Accurate predictions can potentially help investors make more informed
decisions about buying, selling, or holding stocks, aiding in risk.

2.  Data mining Task

In our project, we will use two data mining tasks to help us predict the
closing price of a stock which involves leveraging different data mining
techniques. Here are two specific methods you can consider:
classification and grouping. Closure is classification and grouping. For
classification, we will train our model to be able to classify whether
the close price is high or low based on a set of attributes such as
volume, open, high, low, length etc. For clustering, our model will
create a set of clusters of attributes Low, Heigh, Open, volume,
adjClose, adjHigh.

3.  Data

Our dataset is from the source:
<https://www.kaggle.com/datasets/shreenidhihipparagi/google-stock-prediction>

Number of Attributes: 14

Number of objects: 1258

Attribute characteristics:

+------------+---------+-----------------------------------------------+
| Attribute  | Data    | Description                                   |
| Name       | Type    |                                               |
+------------+---------+-----------------------------------------------+
| symbol     | unique  | Name of company                               |
|            | value   |                                               |
+------------+---------+-----------------------------------------------+
| date       | numeric | date: day, month, and year.                   |
+------------+---------+-----------------------------------------------+
| close      | numeric | closing price of a stock is the final price   |
|            |         | at which a stock is traded on a given trading |
|            |         | day.                                          |
+------------+---------+-----------------------------------------------+
| high       | numeric | The highest price at which a stock traded     |
|            |         | during a specific trading day.                |
+------------+---------+-----------------------------------------------+
| low        | numeric | The lowest price at which a stock traded      |
|            |         | during a specific trading day.                |
+------------+---------+-----------------------------------------------+
| open       | numeric | The price of a stock at the beginning of a    |
|            |         | trading day. It's the price at which the      |
|            |         | first trade occurred on that day.             |
+------------+---------+-----------------------------------------------+
| Volume     | numeric | The total number of shares traded during a    |
|            |         | trading day. Volume is a measure of market    |
|            |         | activity and liquidity for a stock            |
+------------+---------+-----------------------------------------------+
| adjClose   |         | The closing price of a stock adjusted for any |
|            | numeric | corporate actions like dividends, stock       |
|            |         | splits, or other events that could affect the |
|            |         | stock price.                                  |
+------------+---------+-----------------------------------------------+
| adjHigh    | numeric | The highest price of a stock during a trading |
|            |         | day, adjusted for any corporate actions       |
+------------+---------+-----------------------------------------------+
| adjLow     | numeric | The lowest price of a stock during a trading  |
|            |         | day, adjusted for any corporate actions.      |
+------------+---------+-----------------------------------------------+
| adjOpen    | numeric | The opening price of a stock at the beginning |
|            |         | of a trading day, adjusted for any corporate  |
|            |         | actions.                                      |
+------------+---------+-----------------------------------------------+
| adjVolume  | numeric | The trading volume of a stock adjusted for    |
|            |         | any corporate actions. This can provide a     |
|            |         | clearer picture of trandingÂ activity.         |
+------------+---------+-----------------------------------------------+
| divCash    | Binary  | The amount of money paid by a company to its  |
|            |         | shareholders as a portion of its profits.     |
|            |         | Dividends are typically paid on a per-share   |
|            |         | basis                                         |
+------------+---------+-----------------------------------------------+
| s          | Binary  | If a stock undergoes a stock split, the split |
| plitFactor |         | factor indicates the ratio by which the       |
|            |         | shares were split. For instance, a 2-for-1    |
|            |         | split means that for every old share, you now |
|            |         | have 2 new shares.                            |
+------------+---------+-----------------------------------------------+

```{r}
# Load necessary packages
if (!require(caret)) {
  install.packages("caret")
}
if (!require(cluster)) {
  install.packages("cluster")
}
if (!require(fpc)) {
  install.packages("fpc")
}
if (!require(ggplot2)) {
  install.packages("ggplot2")
}
library(caret)
library(cluster)
library(fpc)
library(ggplot2)
```

-   **Sample of row**

```{r}
dataset = read.csv('GOOG.csv') 
```

```{r}
View(dataset)
```

*we removed the attributes (symbol, divCash, splitFactor) as they have
one value only so we do not need them*

```{r}
dataset=dataset[,2:12]
```

*Convert the date column to a date format*

```{r}
dataset$date <- as.Date(dataset$date, format = "%Y-%m-%d %H:%M:%S")
```

```{r}
print(dataset)
str(dataset)
```

-   **Statiscal summarise**

```{r}
summary(dataset)
```

*mean of closing price Using the mean closing price can serve as a basic
reference point or a simple benchmark for forecasting future stock
prices. The mean closing price is the average price at which a stock has
closed over a specific period.*

```{r}
mean(dataset$close)
```

**variance Code**

*The concept of variance in the context of closing prices for stock
prediction serves to quantify the spread or dispersion of the closing
prices around their mean or average value. It provides a measure of how
much the actual closing prices deviate from the average closing price
over a specific period.*

```{r}
var(dataset$close)
```

##  Graphs codes

**Scatter Plot**

*This scatterplot helps us to determine whether the closing price and
volume are correlated to each other or not, it shows that the two
attributes are corelated and have proportional relationship.*

```{r}
with(dataset, plot(volume, close))
```

**Barplot**

*The Bar plot
represents the closing price and date in dataset. It indicates that
closing
prices at the end of a traded day are increasing or decreasing depending
on the
date.*

```{r}
barplot(height = dataset$close, names.arg = dataset$date, xlab = "Date", ylab = "Closing price", main = "date vs Close")
```

**Histogram for Close**

*This Histogram
represents the frequency of a stock closing price in the dataset. After
observation, we noticed that the most values lie in between 1000 to
1200.*

```{r}
hist(dataset$close)
```

4.  Data preprocessing

# Data preprocessing

-    **Checking for missing values**

Description for data cleaning:

Data cleaning, including handling missing values like NULLs, is crucial
before utilizing data for analysis or modeling.

Here's why it's important:

Quality of Analysis:

Accuracy: Missing or incorrect data can skew analysis, leading to
inaccurate insights or predictions.

Reliability: Clean data ensures the reliability of your findings,
reducing the risk of making decisions based on flawed information.

**Data cleaning**

*to find the total null values in the dataset #Checking NULL, FALSE
means no null, TRUE cells means the value of the cell is null*

```{r}
is.na(dataset)
sum(is.na(dataset))

print("Since there is no NULL values we don't need to remove any rows")
```

-    **Detecting and removing the outliers**

    removing close outlier

    *First, we identified all outliers in the closed feature and volume.
    Second, we deleted outliers where We find outliers to produce a more
    accurate data set that helps us get more accurate Results later.*

```{r}
library(outliers)

OutClose = outlier(dataset$close, logical =TRUE)
sum(OutClose)
Find_outlier = which(OutClose ==TRUE, arr.ind = TRUE)
OutClose
Find_outlier
```

removing volume's outlier

```{r}
OutVolume = outlier(dataset$volume, logical =TRUE)
sum(OutVolume)
Find_outlier = which(OutVolume ==TRUE, arr.ind = TRUE)
OutVolume
Find_outlier
```

Remove outlier

```{r}
dataset= dataset[-Find_outlier,]
```

**data before preprocessing**

Before preprocessing the data for stock price prediction, several
initial steps are undertaken to understand, review, and prepare the
dataset. These steps are crucial for getting an overview of the data and
assessing its quality and relevance for the prediction task.

```{r}
View(dataset)
print(dataset)
```

-   **transformation**

**Feature selection**

Remove Redundant Features

```{r}
# load the library        
library(mlbench)
library(caret)
library(ggplot2)
library(lattice)

# calculate correlation matrix
correlationMatrix <- cor(dataset[,2:11])

# summarize the correlation matrix
print(correlationMatrix)

# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.5 )

# print indexes of highly correlated attributes
print(highlyCorrelated)
```

-   **Normalization**

normalization was performed to ensure consistent scaling of the data.
The normalization technique applied was the max-min normalization. This
technique rescales the values of specific attributes within a defined
range between 0 and 1.

We can use the normalized dataset provides a more uniform and comparable
representation of the attributes, enabling accurate analysis and
modeling for stock predaction with result as shown.

```{r}
normalize <- function(x) {return ((x - min(x)) / (max(x) - min(x)))}
dataWithoutNormalization <- dataset
dataset$close<-normalize(dataWithoutNormalization$close)
dataset$volume<-normalize(dataWithoutNormalization$volume)
dataset$open<-normalize(dataWithoutNormalization$open)
dataset$low <-normalize(dataWithoutNormalization$low)
dataset$high <-normalize(dataWithoutNormalization$high)
```

-   **Discretization**

we used the Discretization technique on our class label "close" to
simplify it as it has a large continuous values, we made them fall into
intervals, to make it easier to analyze

and we chose the value 0.2957251 as it the mean value for the closing

```{r}
dataset$close <- ifelse(dataset$close <= 0.2957251 , "low","High")
print(dataset)
```

we discretized it into two categories (low, high) based on the maen, low
meaning it is less than the mean of the close , and high meaning it is
equal to or higher than the mean.

#Encoding We encoded close data into factors, which would help the model
read this data easily

```{r}

dataset$close <- factor(dataset$close,levels = c("low", "High"), labels = c("1", "2"))

print(dataset)
```

#summary after preprocessing after preprocessing the data for stock
price prediction, several steps are taken to refine, clean, and prepare
the data for analysis and modeling. These preprocessing steps aim to
enhance the quality and reliability of the data for more accurate stock
price prediction.

```{r}
print(dataset)
View(dataset)
```

**Feature selection**

Feature selection is a process of selecting a subset of relevant
features (or attributes) from the original set of features in a dataset.
The goal of feature selection is to choose the most relevant and
important features, thereby reducing dimensionality, and improving model
performance.

#Feature selection ,Feature selection using Recursive Feature
Elimination or RFE

```{r}
    library(mlbench)
library(caret)

# define the control using a random forest selection function 
# number=12 means the length of the list
control <- rfeControl(functions=rfFuncs, method="cv", number=11)
# run the RFE algorithm from column 1 to 11  
results <- rfe(dataset[,1:10],dataset[,11], sizes=c(1:10), rfeControl=control)
```

summarize the results

```{r}
print(results)
```

list the chosen features

```{r}
predictors(results)
```

plot the results

```{r}
plot(results, type=c("h", "o"))
```

5.   **Data Mining Technique**

We did both supervised and unsupervised learning techniques on our
dataset (Google stock prediction), which involves classification and
clustering methods, for classification we did a partitioning method
called the train-test split, which splits the dataset into two subsets
of different ratios, and we implemented three algorithms to form 9
different decision trees.

-   **Classification**

We will choose the attributes with the highest importance (from feature
selection) to create a tree:

1.  Dividing the dataset:

we divided our dataset into two divisions for each split:

first one 70-30, which means Training(70%) and Testing(30%):

```{r}
# a fixed random seed to make results reproducible
set.seed(1234)

# 1.Split the datasets into two subsets: Training(70%) and Testing(30%):
ind1 <- sample(2, nrow(dataset), replace=TRUE, prob=c( 0.70, 0.30))
trainData  <- dataset[ind1==1,]
testData <- dataset[ind1==2,]
```

#### 2.Determine the predictor attributes and the class label attribute.( the formula):

```{r}
library(party)    
#myFormula 
myFormula <- close ~volume+open+high+low

```

1.  **Build a decision tree using Information gain:**

Information gain is a concept used in the field of machine learning and
decision tree algorithms. It is a measure of the effectiveness of a
particular attribute in classifying data. In the context of decision
trees, information gain helps determine the order in which attributes
are chosen for splitting the data.

```{r}
dataset_ctree <- ctree(myFormula, data=trainData)
table(predict(dataset_ctree), trainData$close)
# 4.Print and plot the tree:

print(dataset_ctree)
plot(dataset_ctree, type="simple")
```

```{r}
# 5.Use the constructed model to predict the class labels of test data:
testPred <- predict(dataset_ctree, newdata = testData)
result<-table(testPred, testData$close)
result
```

```{r}
# Evaluate the model and create confusion matrix
install.packages("caret")
install.packages('e1071', dependencies=TRUE)
library(e1071)
library(caret)

co_result <- confusionMatrix(result)

print(co_result)
sensitivity(as.table(co_result))
specificity(as.table(co_result))
precision(as.table(co_result))

acc <- co_result$overall["Accuracy"]
acc
```

2.  **Building the Tree using Gini Index(CART)**

The Gini Index is another criterion used in decision tree algorithms,
particularly in the context of the Classification and Regression Trees
(CART) algorithm. Like information gain, the Gini Index is used to
evaluate the impurity or homogeneity of a dataset.

The Gini Index for a specific attribute measures the probability of
incorrectly classifying a randomly chosen element in the dataset. A
lower Gini Index indicates a purer or more homogeneous set. In the
context of decision trees, the attribute with the lowest Gini Index is
chosen as the split attribute.

```{r}
# For decision tree model
install.packages("rpart")
library(rpart)
# For data visualization
library(rpart.plot)

dataset.cart <- rpart(myFormula, data = trainData, method = "class", parms = list(split = "gini"))
```

Visualizing the unpruned tree

```{r}
library(rpart.plot)
rpart.plot(dataset.cart)
```

Checking the order of variable importance

```{r}
dataset.cart$variable.importance
pred.tree = predict(dataset.cart, testData, type = "class")

table(pred.tree,testData$close)
```

```{r}
# 5.Use the constructed model to predict the class labels of test data:
testPred <- predict(dataset_ctree, newdata = testData)
result<-table(testPred, testData$close)
result
```

```{r}
# Evaluate the model and create confusion matrix
install.packages("caret")
install.packages('e1071', dependencies=TRUE)
library(e1071)
library(caret)

co_result <- confusionMatrix(result)

print(co_result)
sensitivity(as.table(co_result))
specificity(as.table(co_result))
precision(as.table(co_result))

acc <- co_result$overall["Accuracy"]
acc
```

3.  **Building the Tree using Gain ratio(C5)**

The Gain Ratio is used to select the attribute that maximizes the
Information Gain while avoiding the bias towards attributes with many
values. It provides a more balanced measure for attribute selection in
decision tree construction.

While Information Gain simply measures the reduction in entropy or
uncertainty, Gain Ratio takes into account the intrinsic information of
an attribute. It aims to penalize attributes that may have a large
number of values, potentially leading to overfitting.

```{r}
install.packages("caret")
install.packages("C50")
install.packages("printr")

library(C50)
library(printr)
library(caret)
#train using the trainData and create the c5.0 gain ratio tree
CloseTree <- C5.0(myFormula, data=trainData)
summary(CloseTree)
plot(CloseTree)
```

second one 60-40, which means Training(60%) and Testing(40%):

```{r}
# a fixed random seed to make results reproducible
set.seed(1234)

# 1.Split the datasets into two subsets: Training(60%) and Testing(40%):
ind1 <- sample(2, nrow(dataset), replace=TRUE, prob=c(0.60 , 0.40))
trainData  <- dataset[ind1==1,]
testData <- dataset[ind1==2,]
```

#### 2.Determine the predictor attributes and the class label attribute.( the formula):

```{r}
library(party)    
#myFormula 
myFormula <- close ~volume+open+high+low

```

#### 3.Build a decision tree using training set and check the Prediction:

```{r}
dataset_ctree <- ctree(myFormula, data=trainData)
table(predict(dataset_ctree), trainData$close)
# 4.Print and plot the tree:

print(dataset_ctree)
plot(dataset_ctree, type="simple")
```

```{r}
# 5.Use the constructed model to predict the class labels of test data:
testPred <- predict(dataset_ctree, newdata = testData)
result<-table(testPred, testData$close)
result
```

```{r}
# Evaluate the model and create confusion matrix
install.packages("caret")
install.packages('e1071', dependencies=TRUE)
library(e1071)
library(caret)

co_result <- confusionMatrix(result)

print(co_result)
sensitivity(as.table(co_result))
specificity(as.table(co_result))
precision(as.table(co_result))

acc <- co_result$overall["Accuracy"]
acc
```

2.  Building the Tree using Gini Index(CART)

```{r}
# For decision tree model
install.packages("rpart")
library(rpart)
# For data visualization
library(rpart.plot)

dataset.cart <- rpart(myFormula, data = trainData, method = "class", parms = list(split = "gini"))
```

# Visualizing the unpruned tree

```{r}
rpart.plot(dataset.cart)
```

# Checking the order of variable importance

```{r}
dataset.cart$variable.importance
pred.tree = predict(dataset.cart, testData, type = "class")

table(pred.tree,testData$close)
```

```{r}
# 5.Use the constructed model to predict the class labels of test data:
testPred <- predict(dataset_ctree, newdata = testData)
result<-table(testPred, testData$close)
result
```

```{r}
# Evaluate the model and create confusion matrix
install.packages("caret")
install.packages('e1071', dependencies=TRUE)
library(e1071)
library(caret)

co_result <- confusionMatrix(result)

print(co_result)
sensitivity(as.table(co_result))
specificity(as.table(co_result))
precision(as.table(co_result))

acc <- co_result$overall["Accuracy"]
acc
```

3.  Building the Tree using Gain ratio(C5)

```{r}
install.packages("caret")
install.packages("C50")
install.packages("printr")

library(C50)
library(printr)
library(caret)
#train using the trainData and create the c5.0 gain ratio tree
CloseTree <- C5.0(myFormula, data=trainData)
summary(CloseTree)
plot(CloseTree)
```

Third one 80-20, which means Training(80%) and Testing(20%):

```{r}
# a fixed random seed to make results reproducible
set.seed(1234)

# 1.Split the datasets into two subsets: Training(80%) and Testing(20%):
ind1 <- sample(2, nrow(dataset), replace=TRUE, prob=c(0.80 , 0.20))
trainData  <- dataset[ind1==1,]
testData <- dataset[ind1==2,]
```

# 2.Determine the predictor attributes and the class label attribute.( the formula):

```{r}
library(party)    
#myFormula 
myFormula <- close ~volume+open+high+low

```

3.Build a decision tree using training set and check the Prediction:

```{r}
dataset_ctree <- ctree(myFormula, data=trainData)
table(predict(dataset_ctree), trainData$close)
# 4.Print and plot the tree:

print(dataset_ctree)
plot(dataset_ctree, type="simple")
```

```{r}
# 5.Use the constructed model to predict the class labels of test data:
testPred <- predict(dataset_ctree, newdata = testData)
result<-table(testPred, testData$close)
result
```

```{r}
# Evaluate the model and create confusion matrix
install.packages("caret")
install.packages('e1071', dependencies=TRUE)
library(e1071)
library(caret)

co_result <- confusionMatrix(result)

print(co_result)
sensitivity(as.table(co_result))
specificity(as.table(co_result))
precision(as.table(co_result))

acc <- co_result$overall["Accuracy"]
acc
```

2.  Building the Tree using Gini Index(CART)

```{r}
# For decision tree model
install.packages("rpart")
library(rpart)
# For data visualization
library(rpart.plot)

dataset.cart <- rpart(myFormula, data = trainData, method = "class", parms = list(split = "gini"))
```

Visualizing the unpruned tree

```{r}
library(rpart.plot)
rpart.plot(dataset.cart)
```

Checking the order of variable importance

```{r}
dataset.cart$variable.importance
pred.tree = predict(dataset.cart, testData, type = "class")

table(pred.tree,testData$close)
```

```{r}
# 5.Use the constructed model to predict the class labels of test data:
testPred <- predict(dataset_ctree, newdata = testData)
result<-table(testPred, testData$close)
result
```

```{r}
# Evaluate the model and create confusion matrix
install.packages("caret")
install.packages('e1071', dependencies=TRUE)
library(e1071)
library(caret)

co_result <- confusionMatrix(result)

print(co_result)
sensitivity(as.table(co_result))
specificity(as.table(co_result))
precision(as.table(co_result))

acc <- co_result$overall["Accuracy"]
acc
```

3.  Building the Tree using Gain ratio(C5)

```{r}
install.packages("caret")
install.packages("C50")
install.packages("printr")

library(C50)
library(printr)
library(caret)
#train using the trainData and create the c5.0 gain ratio tree
CloseTree <- C5.0(myFormula, data=trainData)
summary(CloseTree)
plot(CloseTree)
```

## Discussion:

6.   Evaluation and Comparison

after doing all the three methods we have noticed that in IG and Gini
Index(CART)

the Training(70%) and Testing(30%) has sensitivity = 0.9959016
specificity = 0.9685039 Accuracy = 0.9865229

the Training(60%) and Testing(40%) has sensitivity = 0.9969512
specificity = 0.9710983 Accuracy = 0.988024

the Training(80%) and Testing(20%) has sensitivity = 0.9940476
specificity = 0.9655172 Accuracy = 0.9843137

which means that the best spilting in our dataset is the *Training(60%)
and Testing(40%)* because it is has the highest sensitivity = 0.9940476
%99.4 , specificity = 0.9655172 %96.5 , Accuracy = 0.988024 %98.8

-   **Clustring**

    clustering is an unsupervised learning, it doesn\'t use a class
    label for implementing the cluster. To implement the clusters we
    used the K-mean algorithm, which is an algorithm that produces K
    clusters, which each cluster is represented by the center point of
    the cluster and assigns each object to the nearest cluster, then
    iteratively recalculates the center, and reassigns the object until
    the center point of each cluster does not change that means the
    object in the right cluster.

    factoextra packages is used to help in implementing the clustering
    technique.

    scale() method is used for scaling and centering of data set
    objects, Kmeans() method to find a specified number of clusters.
    fviz_cluster() method to

    visualize the clusters diagram. silhouette() method to calculate the
    average for each cluster, fviz_silhouette() to visualize it, and
    fviz_nbclust() method to set a comparison between the three
    different numbers of clusters to find the optimal number by
    evaluating the clusters according to how well the clusters are
    separated, and how compact the clusters are. In both techniques, we
    used the method set.seed() with the same random number each time we
    try

    a different size to ensure that we get the same result each time.

```{r}
# prepreocessing 
#Data types should be transformed into numeric types before clustering.
dataset<-dataset[,3:11]
dataset <- scale(dataset)
View(dataset)
```

Apply k-means clustering for different values of K

```{r}
# k-means clustering to find 4 clusters 
#set a seed for random number generation  to make the results reproducible
set.seed(8953)
kmeans.result <- kmeans(dataset, 4)

# print the clustering result
print(kmeans.result)
```

```{r}
# visualize clustering
#install.packages("factoextra")
library(factoextra)
fviz_cluster(kmeans.result, data = dataset)

#average silhouette for each clusters 
library(cluster)
avg_sil <- silhouette(kmeans.result$cluster,dist(dataset)) #a dissimilarity object inheriting from class dist or coercible to one. If not specified, dmatrix must be.
fviz_silhouette(avg_sil)#k-means clustering with estimating k and initializations

# Total sum of squares
kmeans.result$tot.withinss

#bcubed metrix that take the avg of precision&recall
library('DPBBM')
c = kmeans.result$cluster
BCubed_metric(kmeans.result$cluster, 0.5)
```

```{r}
# visualize clustering
#install.packages("factoextra")
library(factoextra)
fviz_cluster(kmeans.result, data = dataset)
```

```{r}
# run k-means clustering to find 3 clusters
#set a seed for random number generation  to make the results reproducible
set.seed(8953)
kmeans.result <- kmeans(dataset, 3)

# print the clustering result
print(kmeans.result)
```

```{r}
# visualize clustering
#install.packages("factoextra")
library(factoextra)
fviz_cluster(kmeans.result, data = dataset)

#average silhouette for each clusters 
library(cluster)
avg_sil <- silhouette(kmeans.result$cluster,dist(dataset)) #a dissimilarity object inheriting from class dist or coercible to one. If not specified, dmatrix must be.
fviz_silhouette(avg_sil)#k-means clustering with estimating k and initializations

# Total sum of squares
kmeans.result$tot.withinss

#bcubed metrix that take the avg of precision&recall
library('DPBBM')
c = kmeans.result$cluster
BCubed_metric(kmeans.result$cluster, 0.5)
```

```{r}
#4- visualize clustering
#install.packages("factoextra")
library(factoextra)
fviz_cluster(kmeans.result, data = dataset)
```

```{r}
# run k-means clustering to find 2 clusters
#set a seed for random number generation  to make the results reproducible
set.seed(8953)
kmeans.result <- kmeans(dataset, 2)

# print the clustering result
print(kmeans.result)
```

```{r}
#4- visualize clustering
#install.packages("factoextra")
library(factoextra)
fviz_cluster(kmeans.result, data = dataset)
```

```{r}
install.packages("fpc")
library(fpc)
#kmeansruns() : It calls  kmeans() to perform  k-means clustering
#It initializes the k-means algorithm several times with random points from the data set as means.
#It estimates the number of clusters by index or average silhouette width
kmeansruns.result <- kmeansruns(dataset)  
kmeansruns.result
fviz_cluster(kmeansruns.result, data = dataset)
```

k-mediods clustering with PAM

```{r}
#install.packages("cluster")
library(cluster)
# group into 4 clusters
pam.result <- pam(dataset, 4)
plot(pam.result)
```

Hierarchical Clustering

```{r}
##----Hierarchical Clustering of the Data-----##
set.seed(2835)
# draw a sample of 40 records from the dataset data, so that the clustering plot will not be over crowded
idx <- sample(1:dim(dataset)[1], 40)
dataset2 <- dataset[idx, ]
## hiercrchical clustering
library(factoextra) 
hc.cut <- hcut(dataset2, k = 2, hc_method = "complete") # Computes Hierarchical Clustering and Cut the Tree

```

```{r}
# Visualize dendrogram
fviz_dend(hc.cut,rect = TRUE)  #logical value specifying whether to add a rectangle around groups.
# Visualize cluster
fviz_cluster(hc.cut, ellipse.type = "convex") # Character specifying frame type. Possible values are 'convex', 'confidence' etc

```

Validation

```{r}
#average silhouette for each clusters 
library(cluster)
avg_sil <- silhouette(kmeans.result$cluster,dist(dataset)) #a dissimilarity object inheriting from class dist or coercible to one. If not specified, dmatrix must be.
fviz_silhouette(avg_sil)#k-means clustering with estimating k and initializations

```

define function to compute average silhouette for k clusters using
silhouette()

```{r}
silhouette_score <- function(k){ 
  km <- kmeans(USArrests, centers = k,nstart=25) # if centers is a number, how many random sets should be chosen?
  ss <- silhouette(km$cluster, dist(USArrests))
  sil<- mean(ss[, 3])
  return(sil)
}
```

```{r}
# k cluster range from 2 to 10
k <- 2:10
##  call  function fore k value
avg_sil <- sapply(k, silhouette_score)  ##Apply a Function over a List or Vector
plot(k, type='b', avg_sil, xlab='Number of clusters', ylab='Average Silhouette Scores', frame=FALSE)

```

silhouette method

```{r}
#install.packages("NbClust")
library(NbClust)
#a)fviz_nbclust() with silhouette method using library(factoextra) 
fviz_nbclust(dataset, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")
```

```{r}
#b) NbClust validation
fres.nbclust <- NbClust(dataset, distance="euclidean", min.nc = 2, max.nc = 10, method="kmeans", index="all")
```

```{r}
# Elbow method for determining the optimal number of clusters (k-means)
wss <- numeric(length = 10)
for (k in 1:10) {
  kmeans_model <- kmeans(dataset, centers = k, nstart = 10)
  wss[k] <- sum(kmeans_model$close)
}
```

after doing 3 sizes of k and based the plot and drawing we have noticed
that The best size is K 2 , it is Partition better than the other

```{r}
# Extract the total within-cluster sum of squares (TWSS)
twss <- sum(kmeans.result$withinss)
# Print the TWSS
cat(paste("Total Within-Cluster Sum of Squares (TWSS):", twss, "\n"))
```

```{r}
# Evaluate BCubed precision and recall for k-medoids
# Install and load required libraries
library(caret)
library(ggplot2)
library(lattice)


# Assuming you have true labels and predicted labels
true_labels <- c(1, 1, 1, 0, 0, 1, 0, 1, 0, 1)
predicted_labels <- c(1, 0, 1, 0, 0, 1, 0, 1, 1, 1)

# Create a confusion matrix
conf_matrix <- confusionMatrix(factor(predicted_labels), factor(true_labels))

# Extract recall from the confusion matrix
recall <- conf_matrix$byClass["Sensitivity"]

# Print the result
cat(paste("Recall:", recall, "\n"))
```

*since our dataset is numeric and after doing the clustering and
Classification we have noticed that the clustering fits more for the
dataset because it's concept all about the numeric data*
